{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fc66d223",
      "metadata": {
        "id": "fc66d223"
      },
      "source": [
        "\n",
        "# ðŸ‡®ðŸ‡© Sentiment Analysis (Bahasa Indonesia) â€” Enhanced (Per Bagian)\n",
        "Notebook ini menggabungkan *pipeline lama* dengan **tiga peningkatan** yang diminta:\n",
        "1) Normalisasi **singkatan** (gkâ†’tidak, tpâ†’tapi, dll)  \n",
        "2) Konversi **emoji â†’ kata** (ðŸ™‚â†’\"senyum\", ðŸ˜¢â†’\"sedih\", dsb)  \n",
        "3) Pembuangan **kata sangat jarang** lewat `min_df` pada TFâ€‘IDF (lebih rapi dibanding hapus manual)\n",
        "\n",
        "Tambahan opsional yang disiapkan (tinggal aktifkan bagian yang dikomentari):\n",
        "- **Char nâ€‘grams** (menolong typo & ejaan informal)\n",
        "- **Stopword removal + Sastrawi stemming**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7b250de",
      "metadata": {
        "id": "d7b250de"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# Setup & Paths\n",
        "# ============================================================\n",
        "\n",
        "# (Opsional) Jalankan di Colab untuk mount Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Lokasi CSV (ubah sesuai kebutuhan)\n",
        "# Contoh Drive: '/content/drive/My Drive/Proyek/Data/tweet.csv'\n",
        "CSV_PATH = '/content/drive/My Drive/Proyek/Data/tweet.csv'\n",
        "\n",
        "# Jika file CSV tidak ditemukan di path di atas, fallback ke file lokal (untuk demo)\n",
        "if not os.path.exists(CSV_PATH):\n",
        "    alt = '/mnt/data/tweet.csv'\n",
        "    if os.path.exists(alt):\n",
        "        CSV_PATH = alt\n",
        "\n",
        "# Direktori output model\n",
        "OUTPUT_DIR = '/content/drive/My Drive/Proyek/OutputSentimentNew'\n",
        "if not os.path.exists('/content/drive'):\n",
        "    # fallback lokal jika tidak di Colab/Drive\n",
        "    OUTPUT_DIR = './OutputSentimentNew'\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print('CSV_PATH:', CSV_PATH)\n",
        "print('OUTPUT_DIR:', OUTPUT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1841c9e6",
      "metadata": {
        "id": "1841c9e6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# Install & Import\n",
        "# ============================================================\n",
        "\n",
        "# Jalankan sekali (di Colab) jika paket belum terinstal\n",
        "# !pip -q install emoji Sastrawi\n",
        "\n",
        "import re, unicodedata, zipfile, joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import emoji\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96b9489e",
      "metadata": {
        "id": "96b9489e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# Load Data\n",
        "# ============================================================\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "TEXT_COL, LABEL_COL = \"tweet\", \"sentimen\"\n",
        "\n",
        "print(\"Jumlah baris:\", len(df))\n",
        "print(df.head(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d316c7fe",
      "metadata": {
        "id": "d316c7fe"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# Normalisasi: encoding, emoji -> kata, slang mapping\n",
        "# ============================================================\n",
        "\n",
        "SLANG_MAP = {\n",
        "    \"gk\": \"tidak\", \"ga\": \"tidak\", \"gak\": \"tidak\", \"nggak\": \"tidak\", \"ngga\": \"tidak\",\n",
        "    \"tdk\": \"tidak\", \"engga\": \"tidak\", \"enggak\": \"tidak\",\n",
        "    \"tp\": \"tapi\", \"tpi\": \"tapi\",\n",
        "    \"yg\": \"yang\", \"dr\": \"dari\", \"krn\": \"karena\", \"karna\": \"karena\",\n",
        "    \"dgn\": \"dengan\", \"dg\": \"dengan\",\n",
        "    \"sdh\": \"sudah\", \"udh\": \"sudah\", \"udah\": \"sudah\",\n",
        "    \"blm\": \"belum\",\n",
        "    \"sm\": \"sama\", \"sy\": \"saya\",\n",
        "    \"bgt\": \"banget\", \"bngt\": \"banget\",\n",
        "    \"dlm\": \"dalam\", \"bkn\": \"bukan\", \"utk\": \"untuk\",\n",
        "    \"aja\": \"saja\",\n",
        "    \"jd\": \"jadi\", \"jg\": \"juga\",\n",
        "    \"krg\": \"kurang\", \"skrg\": \"sekarang\",\n",
        "}\n",
        "\n",
        "EMOJI_SPECIAL = {\n",
        "    \":smiling_face_with_smiling_eyes:\": \"senyum\",\n",
        "    \":slightly_smiling_face:\": \"senyum\",\n",
        "    \":grinning_face:\": \"senyum\",\n",
        "    \":face_with_tears_of_joy:\": \"tertawa\",\n",
        "    \":loudly_crying_face:\": \"sedih\",\n",
        "    \":crying_face:\": \"sedih\",\n",
        "    \":pouting_face:\": \"marah\",\n",
        "    \":angry_face:\": \"marah\",\n",
        "    \":red_heart:\": \"cinta\",\n",
        "    \":thumbs_up:\": \"jempol\",\n",
        "    \":fire:\": \"api\",\n",
        "}\n",
        "\n",
        "URL_RE   = re.compile(r\"http\\S+|www\\.\\S+\")\n",
        "MENT_RE  = re.compile(r\"@\\w+\")\n",
        "HASH_RE  = re.compile(r\"#\\w+\")\n",
        "NONAL_RE = re.compile(r\"[^a-z\\s]\")\n",
        "\n",
        "def fix_encoding(text: str) -> str:\n",
        "    text = str(text).replace(\"\\xa0\", \" \").replace(\"Ã‚\", \" \")\n",
        "    text = unicodedata.normalize(\"NFKC\", text)\n",
        "    return text\n",
        "\n",
        "def convert_emoji_to_words(text: str) -> str:\n",
        "    demojized = emoji.demojize(text, delimiters=(\" :\", \": \"))\n",
        "    for k, v in EMOJI_SPECIAL.items():\n",
        "        demojized = demojized.replace(k, f\" {v} \")\n",
        "    demojized = re.sub(r\":([a-z0-9_]+):\", lambda m: \" \" + m.group(1).replace(\"_\", \" \") + \" \", demojized)\n",
        "    return demojized\n",
        "\n",
        "def normalize_slang(text: str) -> str:\n",
        "    tokens = re.findall(r\"\\w+|\\S\", text.lower())\n",
        "    norm = []\n",
        "    for t in tokens:\n",
        "        key = t.lower()\n",
        "        if re.match(r\"^\\w+$\", key) and key in SLANG_MAP:\n",
        "            norm.append(SLANG_MAP[key])\n",
        "        else:\n",
        "            norm.append(t)\n",
        "    return \" \".join(norm)\n",
        "\n",
        "def clean_text_v2(text: str) -> str:\n",
        "    text = fix_encoding(text)\n",
        "    text = text.lower()\n",
        "    text = URL_RE.sub(\" \", text)\n",
        "    text = MENT_RE.sub(\" \", text)\n",
        "    text = HASH_RE.sub(\" \", text)\n",
        "    text = convert_emoji_to_words(text)\n",
        "    text = normalize_slang(text)\n",
        "    text = NONAL_RE.sub(\" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4462fdbc",
      "metadata": {
        "id": "4462fdbc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# Terapkan Cleaning (opsional: stopword & stemming)\n",
        "# ============================================================\n",
        "\n",
        "df[\"clean_tweet\"] = df[TEXT_COL].astype(str).apply(clean_text_v2)\n",
        "\n",
        "# --- OPSIONAL: aktifkan jika ingin stopword removal + stemming ---\n",
        "# from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "# from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "# stop_factory = StopWordRemoverFactory()\n",
        "# stop_remover = stop_factory.create_stop_word_remover()\n",
        "# stemmer = StemmerFactory().create_stemmer()\n",
        "# def indo_stop_stem(text):\n",
        "#     text = stop_remover.remove(text)\n",
        "#     text = stemmer.stem(text)\n",
        "#     return text\n",
        "# df[\"clean_tweet\"] = df[\"clean_tweet\"].apply(indo_stop_stem)\n",
        "\n",
        "df[[TEXT_COL, \"clean_tweet\", LABEL_COL]].head(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c03d7a3e",
      "metadata": {
        "id": "c03d7a3e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# Feature: TF-IDF (min_df untuk buang kata sangat jarang)\n",
        "# (Opsional) gabungan char n-grams untuk typo/ejaan informal\n",
        "# ============================================================\n",
        "\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "word_vectorizer = TfidfVectorizer(\n",
        "    analyzer=\"word\",\n",
        "    ngram_range=(1,2),\n",
        "    min_df=5,        # buang token muncul <5 dokumen\n",
        "    max_df=0.95,\n",
        "    max_features=20000,\n",
        ")\n",
        "\n",
        "# OPSIONAL: aktifkan untuk gabungan char n-grams\n",
        "use_char_ngrams = False\n",
        "char_vectorizer = TfidfVectorizer(\n",
        "    analyzer=\"char\",\n",
        "    ngram_range=(3,5),\n",
        "    min_df=5,\n",
        "    max_df=0.95\n",
        ")\n",
        "\n",
        "X_word = word_vectorizer.fit_transform(df[\"clean_tweet\"])\n",
        "\n",
        "if use_char_ngrams:\n",
        "    X_char = char_vectorizer.fit_transform(df[\"clean_tweet\"])\n",
        "    X = hstack([X_word, X_char]).tocsr()\n",
        "else:\n",
        "    X = X_word\n",
        "\n",
        "y = df[LABEL_COL]\n",
        "print(\"Shape features:\", X.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "621bcfbc",
      "metadata": {
        "id": "621bcfbc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# Split Train/Test\n",
        "# ============================================================\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"Train:\", X_train.shape, \" Test:\", X_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5a81707",
      "metadata": {
        "id": "a5a81707"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# Modeling: Naive Bayes & Logistic Regression\n",
        "# ============================================================\n",
        "\n",
        "# Naive Bayes\n",
        "nb = MultinomialNB(alpha=1.0)\n",
        "nb.fit(X_train, y_train)\n",
        "y_pred_nb = nb.predict(X_test)\n",
        "\n",
        "print(\"=== MultinomialNB ===\")\n",
        "print(\"Accuracy :\", round(accuracy_score(y_test, y_pred_nb), 4))\n",
        "print(\"F1-macro :\", round(f1_score(y_test, y_pred_nb, average=\"macro\"), 4))\n",
        "print(classification_report(y_test, y_pred_nb, digits=3))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_nb))\n",
        "\n",
        "# Logistic Regression\n",
        "lr = LogisticRegression(max_iter=1000, solver=\"liblinear\", class_weight=\"balanced\")\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "\n",
        "print(\"\\n=== Logistic Regression ===\")\n",
        "print(\"Accuracy :\", round(accuracy_score(y_test, y_pred_lr), 4))\n",
        "print(\"F1-macro :\", round(f1_score(y_test, y_pred_lr, average=\"macro\"), 4))\n",
        "print(classification_report(y_test, y_pred_lr, digits=3))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lr))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9541c43",
      "metadata": {
        "id": "d9541c43"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# Save Model & Vectorizer\n",
        "# ============================================================\n",
        "\n",
        "joblib.dump(word_vectorizer, os.path.join(OUTPUT_DIR, \"tfidf_word_vectorizer.joblib\"))\n",
        "if 'use_char_ngrams' in globals() and use_char_ngrams:\n",
        "    joblib.dump(char_vectorizer, os.path.join(OUTPUT_DIR, \"tfidf_char_vectorizer.joblib\"))\n",
        "\n",
        "joblib.dump(nb, os.path.join(OUTPUT_DIR, \"model_nb.joblib\"))\n",
        "joblib.dump(lr, os.path.join(OUTPUT_DIR, \"model_lr.joblib\"))\n",
        "\n",
        "print(\"Saved to:\", OUTPUT_DIR)\n",
        "print(\"Files:\", os.listdir(OUTPUT_DIR))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c40196f9",
      "metadata": {
        "id": "c40196f9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# Inference (Contoh Pemakaian)\n",
        "# ============================================================\n",
        "def predict_texts(texts, model=\"lr\"):\n",
        "    texts = [clean_text_v2(t) for t in texts]\n",
        "    Xw = word_vectorizer.transform(texts)\n",
        "    if 'use_char_ngrams' in globals() and use_char_ngrams:\n",
        "        Xc = char_vectorizer.transform(texts)\n",
        "        from scipy.sparse import hstack\n",
        "        X_ = hstack([Xw, Xc]).tocsr()\n",
        "    else:\n",
        "        X_ = Xw\n",
        "    mdl = lr if model == \"lr\" else nb\n",
        "    return mdl.predict(X_), mdl.predict_proba(X_)\n",
        "\n",
        "sample_texts = [\n",
        "    \"Senang banget! Pelayanannya cepat :)\",\n",
        "    \"Biasa aja sih, standar.\",\n",
        "    \"Kecewa berat, parah nih kualitasnya ðŸ˜¡\"\n",
        "]\n",
        "preds, probs = predict_texts(sample_texts, model=\"lr\")\n",
        "for t, p, pr in zip(sample_texts, preds, probs):\n",
        "    print(f\"- {t} => {p} | proba={np.max(pr):.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a9f9d68",
      "metadata": {
        "id": "8a9f9d68"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# (Opsional) Zip Output Model untuk diunduh\n",
        "# ============================================================\n",
        "zip_path = os.path.join(OUTPUT_DIR, \"OutputSentimentNew_models.zip\")\n",
        "with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "    for fn in os.listdir(OUTPUT_DIR):\n",
        "        fp = os.path.join(OUTPUT_DIR, fn)\n",
        "        if os.path.isfile(fp) and fn.endswith((\".joblib\", \".zip\")) is False:\n",
        "            # lewati file selain joblib (kecuali zip final)\n",
        "            continue\n",
        "        zf.write(fp, arcname=os.path.basename(fp))\n",
        "\n",
        "print(\"ZIP ready:\", zip_path)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
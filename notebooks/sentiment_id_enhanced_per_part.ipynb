{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDxojG8uoEMO"
      },
      "source": [
        "# Sentiment Analysis (Bahasa Indonesia) — Versi Tuned (Per Bagian) — **Tweet Dataset** (Save ke Drive)\n",
        "\n",
        "Konfigurasi dataset & output:\n",
        "- **TEXT_COL** = `tweet`\n",
        "- **LABEL_COL** = `sentimen`\n",
        "- **CSV_PATH** = `\"/content/drive/My Drive/Proyek/Data/\"`\n",
        "- **DATA_PATH** = `os.path.join(CSV_PATH, \"tweet.csv\")`\n",
        "- **OUTPUT_DIR & MODEL_DIR** = `\"/content/drive/My Drive/Proyek/OutputSentimentNew\"` (semua artefak tersimpan di Drive)\n"
      ],
      "id": "oDxojG8uoEMO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLwcZTEhoEMR"
      },
      "source": [
        "## 0) Mount Google Drive (WAJIB)"
      ],
      "id": "MLwcZTEhoEMR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0)_mount_google_drive_(wajib)"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "assert os.path.isdir('/content/drive/My Drive'), \"Drive belum termount. Ulangi mounting di atas.\"\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "0)_mount_google_drive_(wajib)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnHGcqhfoEMS"
      },
      "source": [
        "## 1) Setup: Instalasi Paket (Colab)"
      ],
      "id": "lnHGcqhfoEMS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1)_setup:_instalasi_paket_(colab)"
      },
      "source": [
        "# Jalankan hanya sekali (boleh di-skip kalau sudah terpasang)\n",
        "!pip -q install pandas scikit-learn matplotlib joblib nltk Sastrawi imbalanced-learn\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "1)_setup:_instalasi_paket_(colab)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gue0cNUYoEMT"
      },
      "source": [
        "## 2) Import & Konfigurasi Dasar"
      ],
      "id": "gue0cNUYoEMT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2)_import_&_konfigurasi_dasar"
      },
      "source": [
        "import os, re, json, joblib, numpy as np, pandas as pd\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "from scipy.sparse import hstack\n",
        "from nltk.corpus import stopwords\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.utils.fixes import loguniform\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "\n",
        "# ====== Konfigurasi ======\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Kolom & path dataset\n",
        "TEXT_COL = \"tweet\"\n",
        "LABEL_COL = \"sentimen\"\n",
        "\n",
        "CSV_PATH = \"/content/drive/My Drive/Proyek/Data/\"\n",
        "DATA_PATH = os.path.join(CSV_PATH, \"tweet.csv\")\n",
        "\n",
        "# Output langsung ke Google Drive (folder baru)\n",
        "OUTPUT_DIR = \"/content/drive/My Drive/Proyek/OutputSentimentNew\"\n",
        "MODEL_DIR  = OUTPUT_DIR  # sama saja\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Stopwords Indonesia\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('indonesian'))\n",
        "\n",
        "# Stemmer Sastrawi\n",
        "stemmer = StemmerFactory().create_stemmer()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "2)_import_&_konfigurasi_dasar"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nPeJDvaoEMU"
      },
      "source": [
        "## 3) Load Dataset"
      ],
      "id": "9nPeJDvaoEMU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3)_load_dataset"
      },
      "source": [
        "# Membaca CSV dan drop kolom index lama jika ada\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "if \"Unnamed: 0\" in df.columns:\n",
        "    df = df.drop(columns=[\"Unnamed: 0\"])\n",
        "\n",
        "assert TEXT_COL in df.columns and LABEL_COL in df.columns, f\"Kolom {TEXT_COL} dan/atau {LABEL_COL} tidak ditemukan!\"\n",
        "\n",
        "print(\"Jumlah data:\", len(df))\n",
        "print(\"Distribusi label:\")\n",
        "print(df[LABEL_COL].value_counts())\n",
        "df.head()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "3)_load_dataset"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMqcr08poEMU"
      },
      "source": [
        "## 4) Preprocessing: Slang Normalization, Negation Marking, Hashtag"
      ],
      "id": "DMqcr08poEMU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4)_preprocessing:_slang_normalization,_negation_marking,_hashtag"
      },
      "source": [
        "import re\n",
        "\n",
        "NEGATIONS = {\"tidak\",\"tak\",\"nggak\",\"ga\",\"gak\",\"enggak\",\"bukan\"}\n",
        "\n",
        "SLANG = {\n",
        "    \"bgt\":\"banget\",\"bgtt\":\"banget\",\"bener2\":\"benar-benar\",\"bner\":\"benar\",\n",
        "    \"tp\":\"tapi\",\"yg\":\"yang\",\"ga\":\"gak\",\"gk\":\"gak\",\"ngga\":\"gak\",\"nggak\":\"gak\",\n",
        "    \"sm\":\"sama\",\"dr\":\"dari\",\"krn\":\"karena\",\"krna\":\"karena\",\"krnnya\":\"karena\"\n",
        "}\n",
        "\n",
        "def normalize_elongation(w: str) -> str:\n",
        "    # batasi pengulangan karakter: 'baguuuusss' -> 'baguus'\n",
        "    return re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", w)\n",
        "\n",
        "def normalize_slang(w: str) -> str:\n",
        "    w = normalize_elongation(w)\n",
        "    return SLANG.get(w, w)\n",
        "\n",
        "def extract_hashtags(text: str):\n",
        "    return re.findall(r\"#(\\w+)\", text)\n",
        "\n",
        "def mark_negation(tokens):\n",
        "    out = []\n",
        "    negate = False\n",
        "    for w in tokens:\n",
        "        if w in NEGATIONS:\n",
        "            out.append(w)   # simpan kata negasinya\n",
        "            negate = True\n",
        "            continue\n",
        "        out.append((f\"NEG_{w}\") if negate else w)\n",
        "        if negate:\n",
        "            negate = False  # rule sederhana: negasi hanya memodifikasi token berikutnya\n",
        "    return out\n",
        "\n",
        "def clean_text_sentiment(text: str) -> str:\n",
        "    text = str(text).lower()\n",
        "    hashtags = extract_hashtags(text)  # simpan isi hashtag (tanpa simbol #)\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \" \", text)  # hapus URL\n",
        "    text = re.sub(r\"@\\w+\", \" \", text)  # hapus mention\n",
        "    text = re.sub(r\"#\\w+\", \" \", text)  # hapus simbol #, kontennya sudah di 'hashtags'\n",
        "    text = re.sub(r\"[^a-z\\s]\", \" \", text)  # buang non-alfabet\n",
        "\n",
        "    tokens = [normalize_slang(w) for w in text.split()]\n",
        "\n",
        "    # jangan buang kata negasi meski ada di stopwords\n",
        "    tokens = [w for w in tokens if (w not in stop_words or w in NEGATIONS)]\n",
        "\n",
        "    # stemming terakhir agar bentuk konsisten\n",
        "    tokens = [stemmer.stem(w) for w in tokens]\n",
        "    tokens = mark_negation(tokens)\n",
        "\n",
        "    # tambahkan konten hashtag sebagai token (distem)\n",
        "    tokens += [stemmer.stem(h) for h in hashtags]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Terapkan cleaning\n",
        "df[\"clean_text\"] = df[TEXT_COL].astype(str).apply(clean_text_sentiment)\n",
        "df[[TEXT_COL, \"clean_text\", LABEL_COL]].head(10)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "4)_preprocessing:_slang_normalization,_negation_marking,_hashtag"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyaCyccZoEMV"
      },
      "source": [
        "## 5) Split Data: Train / Test"
      ],
      "id": "HyaCyccZoEMV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5)_split_data:_train_/_test"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = df[\"clean_text\"].values\n",
        "y = df[LABEL_COL].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=SEED\n",
        ")\n",
        "\n",
        "print(\"Train size:\", len(X_train), \"| Test size:\", len(X_test))\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "5)_split_data:_train_/_test"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZDM0Bl3oEMW"
      },
      "source": [
        "## 6) Vectorization: Word TF-IDF + Char TF-IDF"
      ],
      "id": "ZZDM0Bl3oEMW"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6)_vectorization:_word_tf-idf_+_char_tf-idf"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "word_vect = TfidfVectorizer(\n",
        "    ngram_range=(1,2),\n",
        "    max_features=20000,\n",
        "    min_df=2,\n",
        "    sublinear_tf=True\n",
        ")\n",
        "char_vect = TfidfVectorizer(\n",
        "    analyzer=\"char\",\n",
        "    ngram_range=(3,5),\n",
        "    min_df=2,\n",
        "    sublinear_tf=True\n",
        ")\n",
        "\n",
        "X_train_word = word_vect.fit_transform(X_train)\n",
        "X_test_word  = word_vect.transform(X_test)\n",
        "X_train_char = char_vect.fit_transform(X_train)\n",
        "X_test_char  = char_vect.transform(X_test)\n",
        "\n",
        "X_train_all = hstack([X_train_word, X_train_char])\n",
        "X_test_all  = hstack([X_test_word, X_test_char])\n",
        "\n",
        "X_train_all.shape, X_test_all.shape\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "6)_vectorization:_word_tf-idf_+_char_tf-idf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PnwRs7QoEMW"
      },
      "source": [
        "## 7) (Opsional) Oversampling Training Set"
      ],
      "id": "8PnwRs7QoEMW"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7)_(opsional)_oversampling_training_set"
      },
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "apply_oversampling = False  # ubah ke True jika ingin oversampling\n",
        "if apply_oversampling:\n",
        "    ros = RandomOverSampler(random_state=SEED)\n",
        "    X_train_all, y_train = ros.fit_resample(X_train_all, y_train)\n",
        "    print('Oversampling diterapkan.')\n",
        "    from collections import Counter\n",
        "    print('Distribusi label (train) setelah ROS:', Counter(y_train))\n",
        "else:\n",
        "    print('Oversampling tidak diterapkan.')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "7)_(opsional)_oversampling_training_set"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDH-f34JoEMW"
      },
      "source": [
        "## 8) Model Selection: RandomizedSearchCV (LR & NB)"
      ],
      "id": "IDH-f34JoEMW"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8)_model_selection:_randomizedsearchcv_(lr_&_nb)"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.utils.fixes import loguniform\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "\n",
        "# Logistic Regression\n",
        "lr = LogisticRegression(max_iter=1000, solver=\"liblinear\", class_weight=\"balanced\", random_state=SEED)\n",
        "param_lr = {\"C\": loguniform(1e-2, 1e2), \"penalty\": [\"l1\",\"l2\"]}\n",
        "rs_lr = RandomizedSearchCV(lr, param_lr, n_iter=20, scoring=\"f1_macro\", cv=cv, n_jobs=-1, random_state=SEED, verbose=1)\n",
        "rs_lr.fit(X_train_all, y_train)\n",
        "\n",
        "# MultinomialNB\n",
        "nb = MultinomialNB()\n",
        "param_nb = {\"alpha\": loguniform(1e-3, 10)}\n",
        "rs_nb = RandomizedSearchCV(nb, param_nb, n_iter=20, scoring=\"f1_macro\", cv=cv, n_jobs=-1, random_state=SEED, verbose=1)\n",
        "rs_nb.fit(X_train_all, y_train)\n",
        "\n",
        "cands = [(\"LR\", rs_lr.best_estimator_, rs_lr.best_score_), (\"NB\", rs_nb.best_estimator_, rs_nb.best_score_)]\n",
        "best_name, best_model, best_cv = max(cands, key=lambda x: x[2])\n",
        "print(f\"[CV] Best: {best_name} | mean F1-macro: {best_cv:.4f}\")\n",
        "best_model\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "8)_model_selection:_randomizedsearchcv_(lr_&_nb)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waD29O5zoEMX"
      },
      "source": [
        "## 9) Training Ulang Model Terbaik & Evaluasi di Test Set"
      ],
      "id": "waD29O5zoEMX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9)_training_ulang_model_terbaik_&_evaluasi_di_test_set"
      },
      "source": [
        "best_model.fit(X_train_all, y_train)\n",
        "y_pred = best_model.predict(X_test_all)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "print(\"Confusion matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "f1m = f1_score(y_test, y_pred, average=\"macro\")\n",
        "print(f\"F1-macro (test): {f1m:.4f}\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "9)_training_ulang_model_terbaik_&_evaluasi_di_test_set"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSbIr9ZZoEMX"
      },
      "source": [
        "## 10) Error Analysis (Contoh)"
      ],
      "id": "KSbIr9ZZoEMX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10)_error_analysis_(contoh)"
      },
      "source": [
        "import pandas as pd\n",
        "test_df = pd.DataFrame({\n",
        "    \"text\": X_test,\n",
        "    \"clean_text\": [t for t in X_test],\n",
        "    \"true\": y_test,\n",
        "    \"pred\": y_pred\n",
        "})\n",
        "test_df[\"correct\"] = (test_df[\"true\"] == test_df[\"pred\"]).astype(int)\n",
        "print(\"Akurasi sederhana:\", test_df[\"correct\"].mean())\n",
        "\n",
        "# Tampilkan contoh prediksi salah\n",
        "errors = test_df[test_df[\"correct\"]==0].copy()\n",
        "print(\"Total errors:\", len(errors))\n",
        "errors.head(20)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "10)_error_analysis_(contoh)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH0_4Zq6oEMY"
      },
      "source": [
        "## 11) Simpan Model & Vectorizer (Drive)"
      ],
      "id": "xH0_4Zq6oEMY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11)_simpan_model_&_vectorizer_(drive)"
      },
      "source": [
        "import joblib, os, shutil\n",
        "\n",
        "model_path = os.path.join(OUTPUT_DIR, \"model_best.joblib\")\n",
        "word_path  = os.path.join(OUTPUT_DIR, \"tfidf_word.joblib\")\n",
        "char_path  = os.path.join(OUTPUT_DIR, \"tfidf_char.joblib\")\n",
        "\n",
        "joblib.dump(best_model, model_path)\n",
        "joblib.dump(word_vect, word_path)\n",
        "joblib.dump(char_vect, char_path)\n",
        "\n",
        "print(\"Disimpan ke Drive:\", model_path, word_path, char_path)\n",
        "\n",
        "# Buat ZIP seluruh artefak di OUTPUT_DIR\n",
        "zip_path = os.path.join(OUTPUT_DIR, \"artefak_model.zip\")\n",
        "shutil.make_archive(zip_path.replace(\".zip\",\"\"), 'zip', OUTPUT_DIR)\n",
        "print(\"ZIP dibuat:\", zip_path)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "11)_simpan_model_&_vectorizer_(drive)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oVnFNheoEMY"
      },
      "source": [
        "## 12) Inference: Prediksi Data Baru (Single & Batch)"
      ],
      "id": "8oVnFNheoEMY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12)_inference:_prediksi_data_baru_(single_&_batch)"
      },
      "source": [
        "from scipy.sparse import hstack\n",
        "\n",
        "# ----- Single text -----\n",
        "sample_text = \"Filmnya tidak bagus sama sekali, aku kecewa bgt.\"\n",
        "def predict_text(text, model=best_model, word_v=word_vect, char_v=char_vect):\n",
        "    ct = clean_text_sentiment(text)\n",
        "    Xw = word_v.transform([ct])\n",
        "    Xc = char_v.transform([ct])\n",
        "    Xall = hstack([Xw, Xc])\n",
        "    pred = model.predict(Xall)[0]\n",
        "    return pred, ct\n",
        "\n",
        "pred, cleaned = predict_text(sample_text)\n",
        "print(\"Teks:\", sample_text)\n",
        "print(\"Bersih:\", cleaned)\n",
        "print(\"Prediksi:\", pred)\n",
        "\n",
        "# ----- Batch (CSV dengan kolom TEXT_COL) -----\n",
        "# csv_path = os.path.join(CSV_PATH, \"data_baru.csv\")\n",
        "# df_new = pd.read_csv(csv_path)\n",
        "# df_new[\"clean_text\"] = df_new[TEXT_COL].astype(str).apply(clean_text_sentiment)\n",
        "# Xw = word_vect.transform(df_new[\"clean_text\"])\n",
        "# Xc = char_vect.transform(df_new[\"clean_text\"])\n",
        "# Xall = hstack([Xw, Xc])\n",
        "# df_new[\"pred\"] = best_model.predict(Xall)\n",
        "# out_path = os.path.join(OUTPUT_DIR, \"prediksi_baru.csv\")\n",
        "# df_new.to_csv(out_path, index=False)\n",
        "# print(\"Prediksi batch disimpan ke:\", out_path)\n",
        "# df_new.head()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "12)_inference:_prediksi_data_baru_(single_&_batch)"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}